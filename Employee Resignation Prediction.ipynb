{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 員工離職預測 Employee Resignation Prediciton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 環境設定 Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基礎套件\n",
    "import time, random, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.misc import derivative\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, fbeta_score, roc_curve, auc, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "path = 'C:/Users/twnjeu/Desktop/Jeff/AIdea'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 資料讀取 Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('train.csv')\n",
    "te = pd.read_csv('test.csv')\n",
    "se = pd.read_csv('season.csv')\n",
    "sm = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 資料處理 Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Seasonal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se['se_btrip'] = se['出差數A'] + se['出差數B']\n",
    "se['se_leave'] = se['請假數A'] + se['請假數B']\n",
    "se['se_trilev'] = se['se_btrip'] + se['se_leave']\n",
    "se['se_all'] = se['se_trilev'] + se['加班數']\n",
    "se['se_btrip_rt'] = se['se_btrip']/se['se_all']\n",
    "se['se_leave_rt'] = se['se_leave']/se['se_all']\n",
    "se['se_overtime_rt'] = se['加班數']/se['se_all']\n",
    "se['se_trilev_rt'] = se['se_trilev']/se['se_all']\n",
    "se.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_group = se.groupby(['PerNo', 'yyyy']).agg(['sum', 'mean', 'std', 'max', 'min'])\n",
    "se_nm = [e for e in list(se.columns) if e not in ['PerNo', 'yyyy', 'periodQ']]\n",
    "nm = []\n",
    "for i in se_nm:\n",
    "    for j in ['sum', 'mean', 'std', 'max', 'min']:\n",
    "        nm.append(i+'_se_'+j)\n",
    "se_group.columns = nm\n",
    "se_group.fillna(0, inplace = True)\n",
    "se_gp = se_group.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_se_cat = se_gp[se_gp['yyyy'] < 2017]\n",
    "tv_se_cat = se_gp[se_gp['yyyy'] <= 2017]\n",
    "al_se_cat = se_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_se_f = tr_se_cat.drop('yyyy', axis = 1).groupby('PerNo').agg(['mean', 'std'])\n",
    "tv_se_f = tv_se_cat.drop('yyyy', axis = 1).groupby('PerNo').agg(['mean', 'std'])\n",
    "al_se_f = al_se_cat.drop('yyyy', axis = 1).groupby('PerNo').agg(['mean', 'std'])\n",
    "nm = []\n",
    "se_nm = list(tr_se_cat.columns)\n",
    "se_nm = [e for e in list(tr_se_cat.columns) if e not in ['PerNo','yyyy']]\n",
    "\n",
    "for i in se_nm:\n",
    "    for j in ['mean', 'std']:\n",
    "        nm.append(i+'_'+j)\n",
    "tr_se_f.columns = nm\n",
    "tv_se_f.columns = nm\n",
    "al_se_f.columns = nm\n",
    "tr_se_final = tr_se_f.reset_index()\n",
    "tv_se_final = tv_se_f.reset_index()\n",
    "al_se_final = al_se_f.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.concat([tr, te], axis = 0)\n",
    "base = pd.merge(base, se_gp, on = ['PerNo', 'yyyy'], how = 'left')\n",
    "for i in base.columns:\n",
    "    base[i].fillna(base[i].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = base.groupby('PerNo', as_index = False).agg({'yyyy':'min'})\n",
    "min_year.columns = ['PerNo', 'first_year']\n",
    "base = pd.merge(base, min_year, on = 'PerNo', how = 'left')\n",
    "base['seniority'] = base['yyyy'] - base['first_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base['work_exp'] = base['工作資歷1'] + base['工作資歷2'] + base['工作資歷3'] + base['工作資歷4'] + base['工作資歷5']\n",
    "base['seniority_exp'] = base['年資層級A'] + base['年資層級B'] + base['年資層級C']\n",
    "base['training'] = base['訓練時數A'] + base['訓練時數B'] + base['訓練時數C']\n",
    "base['training_A_rt'] = base['訓練時數A']/base['training']\n",
    "base['training_B_rt'] = base['訓練時數B']/base['training']\n",
    "base['training_C_rt'] = base['訓練時數C']/base['training']\n",
    "base['leave_M3'] = base['近三月請假數A'] + base['近三月請假數B']\n",
    "base['leave_Y1'] = base['近一年請假數A'] + base['近一年請假數B']\n",
    "base['leave_rt'] = base['leave_M3']/base['leave_Y1']\n",
    "base['leave_A_rt'] = base['近三月請假數A']/base['近一年請假數A']\n",
    "base['leave_B_rt'] = base['近三月請假數B']/base['近一年請假數B']\n",
    "base['leave_M3_rt'] = base['近三月請假數A']/base['leave_M3']\n",
    "base['leave_Y1_rt'] = base['近一年請假數A']/base['leave_Y1']\n",
    "base['btrip'] = base['出差數A'] + base['出差數B']\n",
    "base['btrip_A_rt'] = base['出差數A']/base['btrip']\n",
    "base['proj_rt'] = base['專案時數']/base['專案總數']\n",
    "base['performance'] = base['年度績效等級A'] + base['年度績效等級B'] + base['年度績效等級C']\n",
    "base['performance_mean'] = base['performance']/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base['age_marry'] = base['年齡層級']*100 + base['婚姻狀況']\n",
    "base['marry_child'] = base['婚姻狀況']*100 + base['眷屬量']\n",
    "base['title_age'] = base['職等']*100 + base['年齡層級']\n",
    "base['manage_age'] = base['管理層級']*100 + base['年齡層級']\n",
    "base['manage_title'] = base['管理層級']*100 + base['職等']\n",
    "base['seniority_cat'] = base['年資層級A']*10000 + base['年資層級B']*100 + base['年資層級C']\n",
    "base['performance_cat'] = base['年度績效等級A']*10000 + base['年度績效等級B']*100 + base['年度績效等級C']\n",
    "base['dept_title'] = base['歸屬部門']*100 + base['職等']\n",
    "base['dept_manage'] = base['歸屬部門']*100 + base['管理層級']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var = ['sex', '工作分類', '職等', '廠區代碼', '管理層級', '工作資歷1', '工作資歷2', '工作資歷3', '工作資歷4', '工作資歷5', '專案總數', '當前專案角色', \n",
    "           '工作地點', '榮譽數', '是否升遷', '升遷速度', '出差集中度', '年度績效等級A', '年度績效等級B', '年度績效等級C', '年齡層級', '婚姻狀況', \n",
    "            '年資層級A', '年資層級B', '年資層級C', '任職前工作平均年數', '最高學歷', '畢業學校類別', '畢業科系類別', '眷屬量', '通勤成本', '歸屬部門', \n",
    "            '加班數_se_sum', 'training', 'btrip', 'work_exp', 'seniority_exp', 'leave_M3', 'leave_Y1', 'age_marry', 'marry_child', 'title_age', 'manage_age', 'manage_title',\n",
    "           'seniority_cat', 'performance_cat', 'dept_title', 'dept_manage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cat_var:\n",
    "    t_encoding = base.groupby(['yyyy', i], as_index = False).agg({'PerStatus':'mean'})\n",
    "    t_encoding['yyyy'] = t_encoding['yyyy'] + 1\n",
    "    t_encoding.columns = ['yyyy', i, i+'_ec']\n",
    "    base = pd.merge(base, t_encoding, on = ['yyyy', i], how = 'left')\n",
    "    base[i+'_ec'].fillna(base[i+'_ec'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Worth of Evidence(WOE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cat_var:\n",
    "    woe = base.groupby(['yyyy', i], as_index = False).agg({'PerStatus': ['count', 'sum']})\n",
    "    woe.columns = ['yyyy', i, 'total', 'bad']\n",
    "    woe['good'] = woe['total'] - woe['bad']\n",
    "    woe['p_bad'] = woe['bad']/woe['bad'].sum()\n",
    "    woe['p_good'] = woe['good']/woe['good'].sum()\n",
    "    woe[i+'_woe'] = np.log1p(woe['p_good']/woe['p_bad'])\n",
    "    woe[i+'_iv'] = (woe['p_good'] - woe['p_bad'])*woe[i+'_woe']\n",
    "    woe['yyyy'] = woe['yyyy'] + 1\n",
    "    woe[i+'_woe'].replace(np.inf, 2, inplace = True)\n",
    "    woe[i+'_iv'].replace(np.inf, 2, inplace = True)\n",
    "    base = pd.merge(base, woe[['yyyy', i, i+'_woe', i+'_iv']], on = ['yyyy', i], how = 'left')\n",
    "    base[i+'_woe'].fillna(base[i+'_woe'].mean(), inplace = True)\n",
    "    base[i+'_iv'].fillna(base[i+'_iv'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ly = base.copy()\n",
    "base_ly['yyyy'] = base['yyyy'] + 1\n",
    "base_ly.set_index(['PerNo', 'yyyy'], inplace = True)\n",
    "base_ly.drop(['PerStatus', 'sex', '任職前工作平均年數', '最高學歷', '畢業學校類別', '畢業科系類別'], axis = 1, inplace = True)\n",
    "nm_o = base_ly.columns\n",
    "nm_n = [i + '_ly' for i in nm_o]\n",
    "base_ly.columns = nm_n\n",
    "base_ly.reset_index(inplace=True)\n",
    "base_f = pd.merge(base, base_ly, on = ['PerNo', 'yyyy'], how = 'left' )\n",
    "for i in base_f.columns:\n",
    "    base_f[i] = base_f[i].fillna(base_f[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = list(base_f.columns)\n",
    "feat.remove('PerStatus')\n",
    "base_f.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x = base_f.loc[base_f['yyyy'] <  2017, feat]\n",
    "va_x = base_f.loc[base_f['yyyy'] == 2017, feat]\n",
    "tv_x = base_f.loc[base_f['yyyy'] <= 2017, feat]\n",
    "te_x = base_f.loc[base_f['yyyy'] == 2018, feat]\n",
    "tr_y = base_f.loc[base_f['yyyy'] <  2017, 'PerStatus']\n",
    "va_y = base_f.loc[base_f['yyyy'] == 2017, 'PerStatus']\n",
    "tv_y = base_f.loc[base_f['yyyy'] <= 2017, 'PerStatus']\n",
    "all_x = base_f[feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Feature of Fist Year, Last Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_var = tr_x.columns.difference(['PerNo', 'FLG', 'yyyy', 'sex', '任職前工作平均年數', '最高學歷', '畢業學校類別', '畢業科系類別'])\n",
    "dif_var = ['訓練時數A', '訓練時數B', '訓練時數C', '生產總額', '榮譽數', '近三月請假數A', '近一年請假數A', '近三月請假數B', '近一年請假數B', '出差數A', '出差數B', '年度績效等級A', '年度績效等級B', '年度績效等級C', '出差集中度','特殊專案佔比', '眷屬量', '通勤成本']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_first = tr_x.groupby('PerNo')[fl_var].first()\n",
    "tr_last = tr_x.groupby('PerNo')[fl_var].last()\n",
    "tr_first.columns = [i + '_ft' for i in fl_var]\n",
    "tr_last.columns = [i + '_lt' for i in fl_var]\n",
    "tr_fl = pd.concat([tr_first, tr_last], axis = 1).reset_index()\n",
    "for i in dif_var:\n",
    "    tr_fl[i+'_dif'] = tr_fl[i+'_lt'] - tr_fl[i+'_ft']\n",
    "    tr_fl[i+'_rt'] = tr_fl[i+'_dif']/tr_fl[i+'_ft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_first = tv_x.groupby('PerNo')[fl_var].first()\n",
    "tv_last = tv_x.groupby('PerNo')[fl_var].last()\n",
    "tv_first.columns = [i + '_ft' for i in fl_var]\n",
    "tv_last.columns = [i + '_lt' for i in fl_var]\n",
    "tv_fl = pd.concat([tv_first, tv_last], axis = 1).reset_index()\n",
    "for i in dif_var:\n",
    "    tv_fl[i+'_dif'] = tv_fl[i+'_lt'] - tv_fl[i+'_ft']\n",
    "    tv_fl[i+'_rt'] = tv_fl[i+'_dif']/tv_fl[i+'_ft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_first = all_x.groupby('PerNo')[fl_var].first()\n",
    "all_last = all_x.groupby('PerNo')[fl_var].last()\n",
    "all_first.columns = [i + '_ft' for i in fl_var]\n",
    "all_last.columns = [i + '_lt' for i in fl_var]\n",
    "all_fl = pd.concat([all_first, all_last], axis = 1).reset_index()\n",
    "for i in dif_var:\n",
    "    all_fl[i+'_dif'] = all_fl[i+'_lt'] - all_fl[i+'_ft']\n",
    "    all_fl[i+'_rt'] = all_fl[i+'_dif']/all_fl[i+'_ft']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Dummy Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_var = ['婚姻狀況', '最高學歷', '畢業學校類別', '畢業科系類別', '工作地點', '工作分類', '職等', '廠區代碼', '管理層級', '歸屬部門', '當前專案角色', '年齡層級', \n",
    "           '年度績效等級A', '年度績效等級B', '年度績效等級C', '年資層級A', '年資層級B', '年資層級C',\n",
    "           'seniority', 'age_marry', 'marry_child', 'title_age', 'manage_age', 'manage_title', 'seniority_cat', 'performance_cat', 'dept_title', 'dept_manage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cat = pd.get_dummies(all_x[dum_var].astype('int').astype('category'))\n",
    "all_dum = pd.concat([all_x, all_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_var = ['yyyy', '婚姻狀況', '最高學歷', '畢業學校類別', '畢業科系類別', '工作地點', '工作分類', '職等', '廠區代碼', '管理層級', '歸屬部門', '當前專案角色', '年齡層級']\n",
    "sum_var = ['專案時數', '專案總數', '訓練時數A', '訓練時數B', '訓練時數C', '生產總額', '榮譽數', '是否升遷', \n",
    "           '近三月請假數A', '近一年請假數A', '近三月請假數B', '近一年請假數B', '出差數A', '出差數B', \n",
    "           '年度績效等級A', '年度績效等級B', '年度績效等級C', 'training', 'leave_M3', 'leave_Y1', 'btrip', 'performance']\n",
    "avg_var = ['專案時數', '專案總數', '特殊專案佔比', '訓練時數A', '訓練時數B', '訓練時數C', '生產總額', '榮譽數', '是否升遷', '升遷速度', \n",
    "           '近三月請假數A', '近一年請假數A', '近三月請假數B', '近一年請假數B', '出差數A', '出差數B', '出差集中度', \n",
    "           '年度績效等級A', '年度績效等級B', '年度績效等級C', '年資層級A', '年資層級B', '年資層級C', '眷屬量', '通勤成本',\n",
    "           'training', 'training_A_rt', 'training_B_rt', 'training_C_rt', \n",
    "           'leave_M3','leave_Y1', 'leave_rt', 'leave_A_rt', 'leave_B_rt', 'leave_M3_rt', 'leave_Y1_rt', 'btrip', 'btrip_A_rt', 'proj_rt', 'performance', 'performance_mean']\n",
    "max_var = ['專案時數', '專案總數', '特殊專案佔比', '訓練時數A', '訓練時數B', '訓練時數C', '生產總額', '榮譽數', '是否升遷', '升遷速度', \n",
    "           '近三月請假數A', '近一年請假數A', '近三月請假數B', '近一年請假數B', '出差數A', '出差數B', '出差集中度', \n",
    "           '年度績效等級A', '年度績效等級B', '年度績效等級C', '年資層級A', '年資層級B', '年資層級C', '眷屬量', '通勤成本',\n",
    "           '工作資歷1', '工作資歷2', '工作資歷3', '工作資歷4', '工作資歷5', '任職前工作平均年數',\n",
    "           'training', 'training_A_rt', 'training_B_rt', 'training_C_rt', \n",
    "           'leave_M3','leave_Y1', 'leave_rt', 'leave_A_rt', 'leave_B_rt', 'leave_M3_rt', 'leave_Y1_rt', 'btrip', 'btrip_A_rt', 'proj_rt', 'performance', 'performance_mean']\n",
    "min_var = ['專案時數', '專案總數', '特殊專案佔比', '訓練時數A', '訓練時數B', '訓練時數C', '生產總額', '榮譽數', '升遷速度', \n",
    "           '近三月請假數A', '近一年請假數A', '近三月請假數B', '近一年請假數B', '出差數A', '出差數B', '出差集中度', \n",
    "           '年度績效等級A', '年度績效等級B', '年度績效等級C', '年資層級A', '年資層級B', '年資層級C', '眷屬量', '通勤成本',\n",
    "           'training', 'training_A_rt', 'training_B_rt', 'training_C_rt', \n",
    "           'leave_M3','leave_Y1', 'leave_rt', 'leave_A_rt', 'leave_B_rt', 'leave_M3_rt', 'leave_Y1_rt', 'btrip', 'btrip_A_rt', 'proj_rt', 'performance', 'performance_mean']\n",
    "std_var = ['專案時數', '專案總數', '特殊專案佔比', '訓練時數A', '訓練時數B', '訓練時數C', '生產總額', '榮譽數', '升遷速度', \n",
    "           '近三月請假數A', '近一年請假數A', '近三月請假數B', '近一年請假數B', '出差數A', '出差數B', '出差集中度', \n",
    "           '年度績效等級A', '年度績效等級B', '年度績效等級C', '年資層級A', '年資層級B', '年資層級C', '眷屬量', '通勤成本',\n",
    "           'training', 'training_A_rt', 'training_B_rt', 'training_C_rt', \n",
    "           'leave_M3','leave_Y1', 'leave_rt', 'leave_A_rt', 'leave_B_rt', 'leave_M3_rt', 'leave_Y1_rt', 'btrip', 'btrip_A_rt', 'proj_rt', 'performance', 'performance_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "i = 0\n",
    "for df in [tr_dum, tv_dum, all_dum]:\n",
    "    df_dummax = df.groupby('PerNo')[all_cat.columns].agg('max')\n",
    "    df_dct = df.groupby('PerNo')[dct_var].agg(pd.Series.nunique)\n",
    "    df_sum = df.groupby('PerNo')[sum_var].agg('sum')\n",
    "    df_avg = df.groupby('PerNo')[avg_var].agg('mean')\n",
    "    df_max = df.groupby('PerNo')[max_var].agg('max')\n",
    "    df_min = df.groupby('PerNo')[min_var].agg('min')\n",
    "    df_std = df.groupby('PerNo')[min_var].agg('std')\n",
    "    df_dct.columns = [i + '_dcnt'for i in dct_var]\n",
    "    df_sum.columns = [i + '_sum' for i in sum_var]\n",
    "    df_avg.columns = [i + '_avg' for i in avg_var]\n",
    "    df_max.columns = [i + '_max' for i in max_var]\n",
    "    df_min.columns = [i + '_min' for i in min_var]\n",
    "    df_std.columns = [i + '_std' for i in std_var]\n",
    "    df_group = pd.concat([df_dummax, df_dct, df_sum, df_avg, df_max, df_min, df_std], axis = 1).reset_index()\n",
    "    df[i] = df_group\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dum = all_dum[all_dum['yyyy'] < 2017]\n",
    "tr_dummax = tr_dum.groupby('PerNo')[all_cat.columns].agg('max')\n",
    "tr_dct = tr_dum.groupby('PerNo')[dct_var].agg(pd.Series.nunique)\n",
    "tr_sum = tr_dum.groupby('PerNo')[sum_var].agg('sum')\n",
    "tr_avg = tr_dum.groupby('PerNo')[avg_var].agg('mean')\n",
    "tr_max = tr_dum.groupby('PerNo')[max_var].agg('max')\n",
    "tr_min = tr_dum.groupby('PerNo')[min_var].agg('min')\n",
    "tr_std = tr_dum.groupby('PerNo')[min_var].agg('std')\n",
    "tr_dct.columns = [i + '_dcnt'for i in dct_var]\n",
    "tr_sum.columns = [i + '_sum' for i in sum_var]\n",
    "tr_avg.columns = [i + '_avg' for i in avg_var]\n",
    "tr_max.columns = [i + '_max' for i in max_var]\n",
    "tr_min.columns = [i + '_min' for i in min_var]\n",
    "tr_std.columns = [i + '_std' for i in std_var]\n",
    "tr_group = pd.concat([tr_dummax, tr_dct, tr_sum, tr_avg, tr_max, tr_min, tr_std], axis = 1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_dum = all_dum[all_dum['yyyy'] <= 2017]\n",
    "tv_dummax = tv_dum.groupby('PerNo')[all_cat.columns].agg('max')\n",
    "tv_dct = tv_dum.groupby('PerNo')[dct_var].agg(pd.Series.nunique)\n",
    "tv_sum = tv_dum.groupby('PerNo')[sum_var].agg('sum')\n",
    "tv_avg = tv_dum.groupby('PerNo')[avg_var].agg('mean')\n",
    "tv_max = tv_dum.groupby('PerNo')[max_var].agg('max')\n",
    "tv_min = tv_dum.groupby('PerNo')[min_var].agg('min')\n",
    "tv_std = tv_dum.groupby('PerNo')[min_var].agg('std')\n",
    "tv_dct.columns = [i + '_dcnt'for i in dct_var]\n",
    "tv_sum.columns = [i + '_sum' for i in sum_var]\n",
    "tv_avg.columns = [i + '_avg' for i in avg_var]\n",
    "tv_max.columns = [i + '_max' for i in max_var]\n",
    "tv_min.columns = [i + '_min' for i in min_var]\n",
    "tv_std.columns = [i + '_std' for i in std_var]\n",
    "tv_group = pd.concat([tv_dummax, tv_dct, tv_sum, tv_avg, tv_max, tv_min, tv_std], axis = 1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_final = pd.concat([df.set_index('PerNo') for df in [tr_x, tr_group, tr_fl, tr_se_final]], axis = 1, join = 'inner').drop(['FLG', 'yyyy'], axis = 1)\n",
    "# va_final = pd.concat([df.set_index('PerNo') for df in [va_x, tv_group, tv_fl, tv_se_final]], axis = 1, join = 'inner').drop(['FLG', 'yyyy'], axis = 1)\n",
    "# tv_final = pd.concat([df.set_index('PerNo') for df in [tv_x, tv_group, tv_fl, tv_se_final]], axis = 1, join = 'inner').drop(['FLG', 'yyyy'], axis = 1)\n",
    "# te_final = pd.concat([df.set_index('PerNo') for df in [all_x[all_x['FLG'] == 'test'], all_group, all_fl, al_se_final]], axis = 1, join = 'inner').drop(['FLG', 'yyyy'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_byyear = all_dum.copy()\n",
    "all_byyear.replace(np.inf, 0, inplace=True)\n",
    "all_byyear.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_byyear = all_byyear[all_byyear['yyyy'] <  2017].set_index(['PerNo', 'yyyy'])\n",
    "va_byyear = all_byyear[all_byyear['yyyy'] == 2017].set_index(['PerNo', 'yyyy'])\n",
    "tv_byyear = all_byyear[all_byyear['yyyy'] <= 2017].set_index(['PerNo', 'yyyy'])\n",
    "te_byyear = all_byyear[all_byyear['yyyy'] == 2018].set_index(['PerNo', 'yyyy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "tr_year = pd.DataFrame(minmax.fit_transform(tr_byyear), columns = tr_byyear.columns)\n",
    "va_year = pd.DataFrame(minmax.transform(va_byyear), columns = tr_byyear.columns)\n",
    "tv_year = pd.DataFrame(minmax.transform(tv_byyear), columns = tr_byyear.columns)\n",
    "te_year = pd.DataFrame(minmax.transform(te_byyear), columns = tr_byyear.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_final = pd.concat([df.set_index('PerNo') for df in [tr_byyear.reset_index(), tr_fl, tr_se_final]], axis = 1, join = 'inner').drop(['yyyy'], axis = 1)\n",
    "# va_final = pd.concat([df.set_index('PerNo') for df in [va_byyear.reset_index(), tv_fl, tv_se_final]], axis = 1, join = 'inner').drop(['yyyy'], axis = 1)\n",
    "# tv_final = pd.concat([df.set_index('PerNo') for df in [tv_byyear.reset_index(), tv_fl, tv_se_final]], axis = 1, join = 'inner').drop(['yyyy'], axis = 1)\n",
    "# te_final = pd.concat([df.set_index('PerNo') for df in [te_byyear.reset_index(), all_fl, al_se_final]], axis = 1, join = 'inner').drop(['yyyy'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 模型建置 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 訓練及驗證樣本 Train and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(tr_year, label = tr_y, free_raw_data = False)\n",
    "d_valid = lgb.Dataset(va_year, label = va_y, free_raw_data = False)\n",
    "d_all = lgb.Dataset(tv_year, label = tv_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 客製化損失函數 Customized Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Find_Optimal_Cutoff(target, predicted):\n",
    "#     \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     target : Matrix with dependent or target data, where rows are observations\n",
    "\n",
    "#     predicted : Matrix with predicted data, where rows are observations\n",
    "\n",
    "#     Returns\n",
    "#     -------     \n",
    "#     list type, with optimal cutoff value\n",
    "        \n",
    "#     \"\"\"\n",
    "#     fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "#     i = np.arange(len(tpr)) \n",
    "#     roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "#     roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "#     return list(roc_t['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function\n",
    "def lgb_fbeta_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = [1 if i >= 0.05 else 0 for i in y_hat]\n",
    "    return 'fbeta', fbeta_score(y_true, y_hat, beta = 1.5), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_lgb(y_pred, dtrain, alpha, gamma):\n",
    "    a,g = alpha, gamma\n",
    "    y_true = dtrain.label\n",
    "    def fl(x,t):\n",
    "        p = 1/(1+np.exp(-x))\n",
    "        return -( a*t + (1-a)*(1-t) ) * (( 1 - ( t*p + (1-t)*(1-p)) )**g) * ( t*np.log(p)+(1-t)*np.log(1-p) )\n",
    "    partial_fl = lambda x: fl(x, y_true)\n",
    "    grad = derivative(partial_fl, y_pred, n=1, dx=1e-6)\n",
    "    hess = derivative(partial_fl, y_pred, n=2, dx=1e-6)\n",
    "    return grad, hess\n",
    "\n",
    "def focal_loss_lgb_eval_error(y_pred, dtrain, alpha, gamma):\n",
    "    a,g = alpha, gamma\n",
    "    y_true = dtrain.label\n",
    "    p = 1/(1+np.exp(-y_pred))\n",
    "    loss = -( a*y_true + (1-a)*(1-y_true) ) * (( 1 - ( y_true*p + (1-y_true)*(1-p)) )**g) * ( y_true*np.log(p)+(1-y_true)*np.log(1-p) )\n",
    "    return 'focal_loss', np.mean(loss), False\n",
    "\n",
    "\n",
    "def focal_loss_lgb_f1_score(preds, lgbDataset):\n",
    "    preds = expit(preds)\n",
    "    binary_preds = [int(p>0.5) for p in preds]\n",
    "    y_true = lgbDataset.get_label()\n",
    "    return 'f1', f1_score(y_true, binary_preds), True\n",
    "\n",
    "def focal_loss_lgb_fbeta_score(preds, lgbDataset):\n",
    "    preds = expit(preds)\n",
    "    binary_preds = [int(p>0.5) for p in preds]\n",
    "    y_true = lgbDataset.get_label()\n",
    "    return 'fbeta', fbeta_score(y_true, binary_preds, beta = 1.5), True\n",
    "\n",
    "focal_loss = lambda x,y: focal_loss_lgb(x, y, 0.25, 1.)\n",
    "eval_error = lambda x,y: focal_loss_lgb_eval_error(x, y, 0.25, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 貝式參數最佳化 Baysian Optimization for LigtGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_lgbm(num_leaves, max_depth, min_data_in_leaf, learning_rate, lambda_l1, lambda_l2):\n",
    "    \n",
    "    params = {\n",
    "      'objective': 'binary',\n",
    "      'bagging_fraction': 0.8,\n",
    "      'feature_fraction': 0.8,\n",
    "      'bagging_freq' : 1,\n",
    "      'bagging_seed' : 2020,\n",
    "      'min_gain_to_split': 0.0001,\n",
    "      'num_threads': 6,\n",
    "      'verbosity': -1,\n",
    "      'random_state': 123\n",
    "    }\n",
    "\n",
    "    params['num_leaves'] = int(round(num_leaves))\n",
    "    params['max_depth'] = int(round(max_depth))\n",
    "    params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "    params['learning_rate']= round(learning_rate, 2)\n",
    "    params['lambda_l1'] = round(lambda_l1, 2)\n",
    "    params['lambda_l2'] = round(lambda_l2, 2)\n",
    "    \n",
    "    rs = {}\n",
    "    clf = lgb.train(params, d_train, valid_sets=[d_train, d_valid],valid_names=['train','valid'],num_boost_round=1000,evals_result=rs,verbose_eval=200,early_stopping_rounds=200)\n",
    "\n",
    "    return -np.min(rs['valid']['binary_logloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_lgbm_focal(num_leaves, max_depth, min_data_in_leaf, learning_rate, lambda_l1, lambda_l2):\n",
    "    \n",
    "    params = {\n",
    "      'objective': 'binary',\n",
    "      'bagging_fraction': 0.8,\n",
    "      'feature_fraction': 0.8,\n",
    "      'bagging_freq' : 1,\n",
    "      'bagging_seed' : 2020,\n",
    "      'min_gain_to_split': 0.01,\n",
    "      'num_threads': 6,\n",
    "      'verbosity': -1,\n",
    "      'random_state': 123\n",
    "    }\n",
    "\n",
    "    params['num_leaves'] = int(round(num_leaves))\n",
    "    params['max_depth'] = int(round(max_depth))\n",
    "    params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "    params['learning_rate']= round(learning_rate, 2)\n",
    "    params['lambda_l1'] = round(lambda_l1, 2)\n",
    "    params['lambda_l2'] = round(lambda_l2, 2)\n",
    "    \n",
    "    rs = {}\n",
    "    clf = lgb.train(params, d_train, valid_sets=[d_train, d_valid],valid_names=['train','valid'],num_boost_round=1000,fobj=focal_loss,feval=eval_error,evals_result=rs,verbose_eval=200,early_stopping_rounds=200)\n",
    "\n",
    "    return -np.min(rs['valid']['focal_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain space-- Range of hyperparameters\n",
    "pds = {\n",
    "       'num_leaves': (20, 70),\n",
    "       'max_depth': (5, 12),\n",
    "       'min_data_in_leaf': (20, 150),\n",
    "       'learning_rate':(0.01, 0.05),\n",
    "       'lambda_l1': (0.01, 1),\n",
    "       'lambda_l2': (0.01, 1)\n",
    "       }\n",
    "\n",
    "# Surrogate model\n",
    "optimizer = BayesianOptimization(hyp_lgbm_focal, pds, random_state = 1)\n",
    "\n",
    "# Optimize\n",
    "optimizer.maximize(init_points = 3, n_iter = 1)\n",
    "best_params = optimizer.max['params']\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 根據貝式參數最佳化結果更新參數 Update Hyperparameter of LightGBM Based on Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "      'objective': 'binary',\n",
    "      'bagging_fraction': 0.8,  \n",
    "      'feature_fraction': 0.8,  \n",
    "      'bagging_freq' : 1,\n",
    "      'bagging_seed' : 2020,\n",
    "      'min_data_in_leaf': 50,\n",
    "      'num_threads': 6,\n",
    "      'verbosity': -1,\n",
    "      'random_state': 123\n",
    "        }\n",
    "\n",
    "params['num_leaves'] = int(round(best_params['num_leaves']))\n",
    "params['max_depth'] = int(round(best_params['max_depth']))\n",
    "params['min_data_in_leaf'] = int(round(best_params['min_data_in_leaf']))\n",
    "params['learning_rate']= round(best_params['learning_rate'], 2)\n",
    "params['lambda_l1'] = round(best_params['lambda_l1'], 2)\n",
    "params['lambda_l2'] = round(best_params['lambda_l2'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(tr_byyear, label = tr_y, free_raw_data = False)\n",
    "d_valid = lgb.Dataset(va_byyear, label = va_y, free_raw_data = False)\n",
    "d_all = lgb.Dataset(tv_byyear, label = tv_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-beta Score\n",
    "fb_vrs = {}\n",
    "fb_clf = lgb.train(params, d_train, valid_sets = [d_train, d_valid], valid_names = ['train', 'valid'], num_boost_round = 1000, verbose_eval = 200, early_stopping_rounds = 200,\n",
    "                 feval = lgb_fbeta_score, evals_result = fb_vrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal loss\n",
    "fc_vrs = {}\n",
    "fc_clf = lgb.train(params, d_train, valid_sets = [d_train, d_valid], valid_names = ['train', 'valid'], num_boost_round = 1000, verbose_eval = 200, early_stopping_rounds = 200,\n",
    "                fobj = focal_loss, feval = eval_error, evals_result = fc_vrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal loss and F1 Score\n",
    "fcf_vrs = {}\n",
    "fcf_clf = lgb.train(params, d_train, valid_sets = [d_train, d_valid], valid_names = ['train','valid'], num_boost_round = 1000, verbose_eval = 200, early_stopping_rounds = 200,\n",
    "                fobj = focal_loss, feval = focal_loss_lgb_f1_score, evals_result = fcf_vrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 全部訓練資料重新訓練模型 Retrain with All Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(tr_byyear, label = tr_y, free_raw_data = False)\n",
    "d_valid = lgb.Dataset(va_byyear, label = va_y, free_raw_data = False)\n",
    "d_all = lgb.Dataset(tv_byyear, label = tv_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-Beta Score\n",
    "iters = 1000\n",
    "frs = {}\n",
    "model = lgb.train(params, d_all, valid_sets = [d_train, d_valid], valid_names = ['train', 'valid'], num_boost_round = iters, verbose_eval = 200, early_stopping_rounds = 200, \n",
    "                  feval = lgb_fbeta_score, evals_result = frs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 92\n",
    "frs = {}\n",
    "model = lgb.train(params, d_all, valid_sets = [d_train, d_valid], valid_names = ['train', 'valid'], num_boost_round = iters, verbose_eval = 200, early_stopping_rounds = 200, \n",
    "                  fobj = focal_loss, feval = eval_error, evals_result = frs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 測試資料評分 Scoring Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(tv_byyear)\n",
    "test_pred = model.predict(te_byyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_pred.min()<0:\n",
    "    print(test_pred.min(), '<0')\n",
    "    te_pred = expit(test_pred)\n",
    "    th = 0.1\n",
    "else:\n",
    "    print(test_pred.min(), '>0')\n",
    "    te_pred = test_pred\n",
    "    th = 0.05\n",
    "sumit = pd.DataFrame({'PerNo': sm['PerNo'], 'PerStatus': te_pred})\n",
    "sumit.to_csv('submit_prob.csv', index = False)\n",
    "sumit.loc[sumit['PerStatus'] >= th, 'PerStatus'] = 1\n",
    "sumit.loc[sumit['PerStatus'] < th, 'PerStatus'] = 0\n",
    "sumit.to_csv('submit.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
